{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVKhpWuHPg9g"
      },
      "source": [
        "This notebook is a modified version of the hyperparameter testing one, designed for testing the effect of various dropout levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpXYXEm6Pg9q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import random\n",
        "import copy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU Stuff\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device = {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRhb-oIiXVUq",
        "outputId": "557db1f8-c194-4284-c192-436ac047cca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOjbv8NQPg9t"
      },
      "outputs": [],
      "source": [
        "class RandomOptimiser(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_layers, dropout_num):\n",
        "        super(RandomOptimiser, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_num)\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, 3)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL5_pLj6Pg9v"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, valid_features, valid_labels, criterion, optimizer, epochs):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_losses = []\n",
        "    valid_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate training metrics\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_outputs = model(train_loader.dataset.tensors[0])\n",
        "            train_loss = criterion(train_outputs, train_loader.dataset.tensors[1])\n",
        "            train_predicted = torch.argmax(train_outputs, dim=1)\n",
        "            train_accuracy = (train_predicted == train_loader.dataset.tensors[1]).float().mean()\n",
        "            valid_outputs = model(valid_features)\n",
        "            valid_loss = criterion(valid_outputs, valid_labels)\n",
        "            valid_predicted = torch.argmax(valid_outputs, dim=1)\n",
        "            valid_accuracy = (valid_predicted == valid_labels).float().mean()\n",
        "\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "        train_accuracies.append(train_accuracy.item())\n",
        "        valid_losses.append(valid_loss.item())\n",
        "        valid_accuracies.append(valid_accuracy.item())\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}')\n",
        "            print(f'Train Accuracy: {train_accuracies[-1]:.4f}, Valid Accuracy: {valid_accuracies[-1]:.4f}')\n",
        "\n",
        "    return train_losses, train_accuracies, valid_losses, valid_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRrFvH6yPg9x"
      },
      "outputs": [],
      "source": [
        "def random_search(train_features, train_labels, valid_features, valid_labels, n_trials=5):\n",
        "    param_space = {\n",
        "        'learning_rate': [0.001],\n",
        "        'batch_size': [64],\n",
        "        'epochs': [200],\n",
        "        'hidden_size1': [32],\n",
        "        'hidden_size2': [64],\n",
        "        'hidden_size3': [32],\n",
        "        'no_hidden_layers': [3],\n",
        "    }\n",
        "\n",
        "    best_valid_accuracy = 0\n",
        "    lowest_valid_loss = float('inf')\n",
        "    best_model_accuracy = None\n",
        "    best_model_loss = None\n",
        "    best_params_accuracy = None\n",
        "    best_params_loss = None\n",
        "    best_no_hl = None\n",
        "\n",
        "    input_size = train_features.shape[1]\n",
        "\n",
        "    for trial in range(n_trials):\n",
        "        current_params = {\n",
        "            'learning_rate': random.choice(param_space['learning_rate']),\n",
        "            'batch_size': random.choice(param_space['batch_size']),\n",
        "            'epochs': random.choice(param_space['epochs']),\n",
        "            'hidden_size1': random.choice(param_space['hidden_size1']),\n",
        "            'hidden_size2': random.choice(param_space['hidden_size2']),\n",
        "            'hidden_size3': random.choice(param_space['hidden_size3']),\n",
        "            'no_hidden_layers': random.choice(param_space['no_hidden_layers']),\n",
        "        }\n",
        "\n",
        "        print(f\"\\nTrial {trial + 1}/{n_trials}, Dropout = {trial*0.1}\")\n",
        "        print(\"Current parameters:\", current_params)\n",
        "\n",
        "        accuracies = []\n",
        "        losses = []\n",
        "        for i in range(3):\n",
        "          print(f\"Sub-trial {i+1}:\")\n",
        "          model = RandomOptimiser(input_size, current_params['hidden_size1'], current_params['hidden_size2'], current_params['hidden_size3'],current_params['no_hidden_layers'],dropout_num=trial*0.1).to(device)\n",
        "          train_dataset = data.TensorDataset(train_features, train_labels)\n",
        "          train_loader = data.DataLoader(train_dataset, batch_size=current_params['batch_size'], shuffle=True)\n",
        "\n",
        "          # Training setup\n",
        "          criterion = nn.CrossEntropyLoss()\n",
        "          optimizer = optim.Adam(model.parameters(), lr=current_params['learning_rate'])\n",
        "\n",
        "          # Train the model\n",
        "          train_losses, train_accuracies, valid_losses, valid_accuracies = train_model(\n",
        "              model, train_loader, valid_features, valid_labels, criterion, optimizer, current_params['epochs']\n",
        "          )\n",
        "\n",
        "          final_valid_accuracy = valid_accuracies[-1]\n",
        "          final_valid_loss = valid_losses[-1]\n",
        "          accuracies.append(final_valid_accuracy)\n",
        "          losses.append(final_valid_loss)\n",
        "\n",
        "        print(f\"Accuracies: {accuracies}\")\n",
        "        print(f\"Losses: {losses}\")\n",
        "        final_valid_accuracy = np.mean(accuracies)\n",
        "        final_valid_loss = np.mean(losses)\n",
        "        print(f\"Average Accuracy: {final_valid_accuracy}\")\n",
        "        print(f\"Average Loss: {final_valid_loss}\")\n",
        "        # Track the best validation accuracy\n",
        "        if final_valid_accuracy > best_valid_accuracy:\n",
        "            best_valid_accuracy = final_valid_accuracy\n",
        "            best_model_accuracy = copy.deepcopy(model)\n",
        "            best_params_accuracy = current_params\n",
        "            print(f\"New best model by Validation Accuracy! Accuracy: {best_valid_accuracy:.4f}\")\n",
        "\n",
        "        # Track the lowest validation loss\n",
        "        if final_valid_loss < lowest_valid_loss:\n",
        "            lowest_valid_loss = final_valid_loss\n",
        "            best_model_loss = copy.deepcopy(model)\n",
        "            best_params_loss = current_params\n",
        "            print(f\"New best model by Validation Loss! Loss: {lowest_valid_loss:.4f}\")\n",
        "\n",
        "    return best_model_accuracy, best_params_accuracy, best_valid_accuracy, best_model_loss, best_params_loss, lowest_valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY016xL4Pg9y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_data(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for line in data:\n",
        "        if line.strip():\n",
        "            split_line = list(map(float, line.split(',')))\n",
        "            features.append(split_line[:-1])\n",
        "            labels.append(int(split_line[-1]))\n",
        "    return torch.tensor(features, dtype=torch.float32, device=device), torch.tensor(labels, dtype=torch.long, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-7IZJDpUPg9z",
        "outputId": "5ac07e9f-9563-41c0-b622-e87a1448f17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1/1\n",
            "\n",
            "Trial 1/6, Dropout = 0.0\n",
            "Current parameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 200, 'hidden_size1': 32, 'hidden_size2': 64, 'hidden_size3': 32, 'no_hidden_layers': 3}\n",
            "Sub-trial 1:\n",
            "Epoch [50/200], Train Loss: 0.6543, Valid Loss: 1.0398\n",
            "Train Accuracy: 0.7671, Valid Accuracy: 0.5308\n",
            "Epoch [100/200], Train Loss: 0.4751, Valid Loss: 0.4921\n",
            "Train Accuracy: 0.8316, Valid Accuracy: 0.8506\n",
            "Epoch [150/200], Train Loss: 0.4464, Valid Loss: 0.8027\n",
            "Train Accuracy: 0.8391, Valid Accuracy: 0.7031\n",
            "Epoch [200/200], Train Loss: 0.4326, Valid Loss: 0.6906\n",
            "Train Accuracy: 0.8623, Valid Accuracy: 0.7812\n",
            "Sub-trial 2:\n",
            "Epoch [50/200], Train Loss: 0.6661, Valid Loss: 0.8285\n",
            "Train Accuracy: 0.7725, Valid Accuracy: 0.6287\n",
            "Epoch [100/200], Train Loss: 0.5065, Valid Loss: 0.6634\n",
            "Train Accuracy: 0.8516, Valid Accuracy: 0.7146\n",
            "Epoch [150/200], Train Loss: 0.4441, Valid Loss: 0.7597\n",
            "Train Accuracy: 0.8214, Valid Accuracy: 0.7019\n",
            "Epoch [200/200], Train Loss: 0.4221, Valid Loss: 0.4288\n",
            "Train Accuracy: 0.8852, Valid Accuracy: 0.8773\n",
            "Sub-trial 3:\n",
            "Epoch [50/200], Train Loss: 0.6522, Valid Loss: 0.8881\n",
            "Train Accuracy: 0.7585, Valid Accuracy: 0.6234\n",
            "Epoch [100/200], Train Loss: 0.4829, Valid Loss: 0.6458\n",
            "Train Accuracy: 0.8260, Valid Accuracy: 0.7699\n",
            "Epoch [150/200], Train Loss: 0.4544, Valid Loss: 0.7247\n",
            "Train Accuracy: 0.8627, Valid Accuracy: 0.7186\n",
            "Epoch [200/200], Train Loss: 0.4392, Valid Loss: 0.5884\n",
            "Train Accuracy: 0.8765, Valid Accuracy: 0.7675\n",
            "Accuracies: [0.7811854481697083, 0.8772584795951843, 0.7674511671066284]\n",
            "Losses: [0.6906303763389587, 0.42877018451690674, 0.5884238481521606]\n",
            "Average Accuracy: 0.808631698290507\n",
            "Average Loss: 0.5692748030026754\n",
            "New best model by Validation Accuracy! Accuracy: 0.8086\n",
            "New best model by Validation Loss! Loss: 0.5693\n",
            "\n",
            "Trial 2/6, Dropout = 0.1\n",
            "Current parameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 200, 'hidden_size1': 32, 'hidden_size2': 64, 'hidden_size3': 32, 'no_hidden_layers': 3}\n",
            "Sub-trial 1:\n",
            "Epoch [50/200], Train Loss: 1.0505, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0506, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0511, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0505, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 2:\n",
            "Epoch [50/200], Train Loss: 1.0523, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0505, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0509, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0505, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 3:\n",
            "Epoch [50/200], Train Loss: 1.0505, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0505, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0506, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0503, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Accuracies: [0.48663246631622314, 0.48663246631622314, 0.48663246631622314]\n",
            "Losses: [1.0486688613891602, 1.0486475229263306, 1.0486760139465332]\n",
            "Average Accuracy: 0.48663246631622314\n",
            "Average Loss: 1.048664132754008\n",
            "\n",
            "Trial 3/6, Dropout = 0.2\n",
            "Current parameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 200, 'hidden_size1': 32, 'hidden_size2': 64, 'hidden_size3': 32, 'no_hidden_layers': 3}\n",
            "Sub-trial 1:\n",
            "Epoch [50/200], Train Loss: 1.0503, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0503, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0504, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 2:\n",
            "Epoch [50/200], Train Loss: 1.0507, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0503, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 3:\n",
            "Epoch [50/200], Train Loss: 1.0502, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0505, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0507, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Accuracies: [0.48663246631622314, 0.48663246631622314, 0.48663246631622314]\n",
            "Losses: [1.048643708229065, 1.04863703250885, 1.0486433506011963]\n",
            "Average Accuracy: 0.48663246631622314\n",
            "Average Loss: 1.0486413637797039\n",
            "\n",
            "Trial 4/6, Dropout = 0.30000000000000004\n",
            "Current parameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 200, 'hidden_size1': 32, 'hidden_size2': 64, 'hidden_size3': 32, 'no_hidden_layers': 3}\n",
            "Sub-trial 1:\n",
            "Epoch [50/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0508, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 2:\n",
            "Epoch [50/200], Train Loss: 1.0519, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0504, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0508, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0504, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 3:\n",
            "Epoch [50/200], Train Loss: 1.0510, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0505, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0505, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Accuracies: [0.48663246631622314, 0.48663246631622314, 0.48663246631622314]\n",
            "Losses: [1.0486481189727783, 1.0486464500427246, 1.0486477613449097]\n",
            "Average Accuracy: 0.48663246631622314\n",
            "Average Loss: 1.048647443453471\n",
            "\n",
            "Trial 5/6, Dropout = 0.4\n",
            "Current parameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 200, 'hidden_size1': 32, 'hidden_size2': 64, 'hidden_size3': 32, 'no_hidden_layers': 3}\n",
            "Sub-trial 1:\n",
            "Epoch [50/200], Train Loss: 1.0513, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0519, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0508, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0507, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 2:\n",
            "Epoch [50/200], Train Loss: 1.0512, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0502, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0519, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Sub-trial 3:\n",
            "Epoch [50/200], Train Loss: 1.0506, Valid Loss: 1.0486\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0508, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0507, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [200/200], Train Loss: 1.0505, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Accuracies: [0.48663246631622314, 0.48663246631622314, 0.48663246631622314]\n",
            "Losses: [1.048651933670044, 1.0486482381820679, 1.0486658811569214]\n",
            "Average Accuracy: 0.48663246631622314\n",
            "Average Loss: 1.048655351003011\n",
            "\n",
            "Trial 6/6, Dropout = 0.5\n",
            "Current parameters: {'learning_rate': 0.001, 'batch_size': 64, 'epochs': 200, 'hidden_size1': 32, 'hidden_size2': 64, 'hidden_size3': 32, 'no_hidden_layers': 3}\n",
            "Sub-trial 1:\n",
            "Epoch [50/200], Train Loss: 1.0505, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [100/200], Train Loss: 1.0519, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n",
            "Epoch [150/200], Train Loss: 1.0506, Valid Loss: 1.0487\n",
            "Train Accuracy: 0.4837, Valid Accuracy: 0.4866\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1263b430eef0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Perform random search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         best_model_accuracy, best_params_accuracy, best_valid_accuracy, best_model_loss, best_params_loss, lowest_valid_loss = random_search(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-15-1160098de75a>\u001b[0m in \u001b[0;36mrandom_search\u001b[0;34m(train_features, train_labels, valid_features, valid_labels, n_trials)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           train_losses, train_accuracies, valid_losses, valid_accuracies = train_model(\n\u001b[0m\u001b[1;32m     50\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           )\n",
            "\u001b[0;32m<ipython-input-11-8dfbd133e0e1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_features, valid_labels, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    TestingData = open('TestingEncoded.csv', 'r').read().split(\"\\n\")[1:]\n",
        "    TrainingData = open('TrainingEncoded.csv', 'r').read().split(\"\\n\")[1:]\n",
        "    ValidationData = open('ValidationEncoded.csv', 'r').read().split(\"\\n\")[1:]\n",
        "\n",
        "    # Process data\n",
        "    train_features, train_labels = process_data(TrainingData)\n",
        "    valid_features, valid_labels = process_data(ValidationData)\n",
        "    test_features, test_labels = process_data(TestingData)\n",
        "\n",
        "    # Number of runs\n",
        "    num_runs = 1\n",
        "\n",
        "    best_model_accuracy_runs = None\n",
        "    best_model_loss_runs = None\n",
        "    best_params_accuracy_runs = None\n",
        "    best_params_loss_runs = None\n",
        "    best_valid_accuracy_runs = 0\n",
        "    best_valid_loss_runs = float('inf')\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "\n",
        "        # Perform random search\n",
        "        best_model_accuracy, best_params_accuracy, best_valid_accuracy, best_model_loss, best_params_loss, lowest_valid_loss = random_search(\n",
        "            train_features, train_labels, valid_features, valid_labels, n_trials=6\n",
        "        )\n",
        "\n",
        "        # Compare across runs for best accuracy\n",
        "        if best_valid_accuracy > best_valid_accuracy_runs:\n",
        "            best_valid_accuracy_runs = best_valid_accuracy\n",
        "            best_model_accuracy_runs = best_model_accuracy\n",
        "            best_params_accuracy_runs = best_params_accuracy\n",
        "\n",
        "        # Compare across runs for lowest validation loss\n",
        "        if lowest_valid_loss < best_valid_loss_runs:\n",
        "            best_valid_loss_runs = lowest_valid_loss\n",
        "            best_model_loss_runs = best_model_loss\n",
        "            best_params_loss_runs = best_params_loss\n",
        "\n",
        "    # Print the best hyperparameters and results\n",
        "    print(\"\\nBest Validation Accuracy Across Runs:\")\n",
        "    print(f\"Validation Accuracy: {best_valid_accuracy_runs:.4f}\")\n",
        "    print(f\"Best Hyperparameters (Accuracy): {best_params_accuracy_runs}\")\n",
        "\n",
        "    print(\"\\nLowest Validation Loss Across Runs:\")\n",
        "    print(f\"Validation Loss: {best_valid_loss_runs:.4f}\")\n",
        "    print(f\"Best Hyperparameters (Loss): {best_params_loss_runs}\")\n",
        "\n",
        "    # Save the models\n",
        "    torch.save(best_model_accuracy_runs.state_dict(), 'best_model_accuracy.pth')\n",
        "    torch.save(best_model_loss_runs.state_dict(), 'best_model_loss.pth')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}